{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897de966-a41e-4e60-8583-4ee0ac6409fd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #455A64;background-color:#ECEFF1;\">\n",
    "본 자료 및 영상 컨텐츠는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 컨텐츠 및 컨텐츠 일부 문구등을 외부에 공개, 게시하는 것을 금지합니다. 특히 자료에 대해서는 저작권법을 엄격하게 적용하겠습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48102f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Scrapy Shell\n",
    "\n",
    "> 프로젝트 작성 후에, 데이터가 정상 크롤링되는지 확인이 되므로 생산성이 떨어짐 <br> \n",
    "별도 Scrapy Shell 로 먼저 정상 크롤링이 되는지를 확인할 수 있음\n",
    "\n",
    "### 1단계: Scrapy Shell 시작하기\n",
    "\n",
    "먼저, 터미널이나 명령 프롬프트에서 Scrapy shell을 시작합니다. 아래 명령어를 입력하세요:\n",
    "\n",
    "```bash\n",
    "scrapy shell 'https://davelee-fun.github.io/'\n",
    "```\n",
    "\n",
    "이 명령은 `https://davelee-fun.github.io/` 웹 페이지에 대한 HTTP 요청을 보내고, 응답을 받아 Scrapy shell에서 사용할 수 있도록 합니다.\n",
    "\n",
    "### 2단계: 웹 페이지 응답 확인하기\n",
    "\n",
    "Scrapy shell이 시작되면, `response` 객체를 통해 웹 페이지의 응답을 확인할 수 있습니다. 예를 들어, 응답의 HTTP 상태 코드를 확인하려면 다음과 같이 입력합니다:\n",
    "\n",
    "```python\n",
    "response.status\n",
    "```\n",
    "\n",
    "웹 페이지의 HTML 소스를 보려면 다음 명령을 사용합니다:\n",
    "\n",
    "```python\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "### 3단계: CSS 선택자와 XPath를 사용하여 데이터 추출하기\n",
    "\n",
    "Scrapy shell을 사용하면 CSS 선택자나 XPath를 사용하여 웹 페이지에서 데이터를 추출할 수 있습니다. 예를 들어, 웹 페이지의 모든 제목을 추출하려면 다음과 같은 명령을 사용할 수 있습니다:\n",
    "\n",
    "CSS 선택자를 사용하는 경우:\n",
    "\n",
    "```python\n",
    "response.css('h1::text').getall()\n",
    "```\n",
    "\n",
    "XPath를 사용하는 경우:\n",
    "\n",
    "```python\n",
    "response.xpath('//h1/text()').getall()\n",
    "```\n",
    "\n",
    "### 4단계: Scrapy shell에서 fetch 사용하기\n",
    "\n",
    "새로운 웹 페이지에 대한 요청을 보내고 싶다면, `fetch()` 함수를 사용할 수 있습니다. 예를 들어, 다른 페이지를 요청하려면:\n",
    "\n",
    "```python\n",
    "fetch('http://davelee-fun.github.io/blog/TEST/index.html')\n",
    "```\n",
    "\n",
    "`fetch()` 함수를 사용한 후, `response` 객체가 새로운 페이지의 응답으로 업데이트됩니다. 이후 2단계와 3단계의 과정을 반복하여 새로운 페이지에서도 데이터를 추출할 수 있습니다.\n",
    "\n",
    "### 5단계: Scrapy Shell 종료하기\n",
    "\n",
    "작업을 마친 후, Scrapy shell을 종료하려면 다음과 같이 `exit()` 함수를 호출합니다:\n",
    "\n",
    "```python\n",
    "exit()\n",
    "```\n",
    "\n",
    "Scrapy shell은 웹 크롤링과 스크래핑 작업을 빠르게 프로토타이핑하고 테스트하는 데 매우 유용합니다. 위의 단계를 따라 `https://davelee-fun.github.io/` 사이트의 구조를 분석하고, 필요한 데이터를 추출하는 방법을 실험해 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ede037",
   "metadata": {},
   "source": [
    "### 1. Scrapy 프로젝트7\n",
    "\n",
    "#### 1. 프로젝트 생성 및 설정\n",
    "\n",
    "1. **프로젝트 생성**\n",
    "   - 새 Scrapy 프로젝트를 생성합니다.\n",
    "     ```bash\n",
    "     scrapy startproject daveleefun_project7\n",
    "     ```\n",
    "     - `daveleefun_project7`라는 이름의 새 Scrapy 프로젝트가 생성됩니다.\n",
    "\n",
    "2. **프로젝트 디렉토리로 이동**\n",
    "   ```bash\n",
    "   cd daveleefun_project7\n",
    "   ```\n",
    "\n",
    "3. **Spider 생성**\n",
    "   ```bash\n",
    "   scrapy genspider multiple_webs davelee-fun.github.io\n",
    "   ```\n",
    "\n",
    "#### 2. items.py 수정\n",
    "\n",
    "- 크롤링할 데이터의 구조를 정의합니다.\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class DaveleefunItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    category = scrapy.Field()\n",
    "    name = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "```\n",
    "\n",
    "#### 3. 여러 페이지 크롤링 로직 구현\n",
    "\n",
    "- Spider 파일에 여러 페이지를 크롤링하는 로직을 추가합니다.\n",
    "```python\n",
    "import scrapy\n",
    "from daveleefun_project7.items import DaveleefunItem\n",
    "\n",
    "class MultipleWebsSpider(scrapy.Spider):\n",
    "    name = 'multiple_webs'\n",
    "    allowed_domains = ['davelee-fun.github.io']\n",
    "    start_urls = ['http://davelee-fun.github.io/']\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = self.start_urls\n",
    "        urls.extend([f'https://davelee-fun.github.io/page{i}' for i in range(2, 7)])\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url, self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        products = response.css('div.card.h-100') # getall() 은 태그를 문자열로만 가져오므로 객체로 가져옴\n",
    "        for product in products:\n",
    "            item = DaveleefunItem()\n",
    "            item['link'] = product.css('div.maxthumb > a::attr(href)').get()\n",
    "            item['category'] = product.css('a.text-dark::text').get()\n",
    "            item['title'] = product.css('h4.card-text::text').get()\n",
    "            # 중첩된 태그 내의 TEXT 를 가져올 수 없음 (즉, 'span.post-name::text' 은 정상동작하지 않음)\n",
    "            item['name'] = product.css('span.post-name a::text').get()\n",
    "            item['date'] = product.css('span.post-date::text').get()\n",
    "            yield item\n",
    "```\n",
    "\n",
    "#### 4. pipelines.py 데이터 전처리 추가\n",
    "\n",
    "- `pipelines.py`에 데이터 전처리 로직을 추가합니다.\n",
    "```python\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "\n",
    "class LinkCompletionPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        # 기본 URL 설정\n",
    "        base_url = 'https://davelee-fun.github.io'\n",
    "        \n",
    "        # 링크 데이터 후처리\n",
    "        if 'link' in item:\n",
    "            # 링크 앞에 기본 URL을 추가\n",
    "            item['link'] = base_url + item['link']\n",
    "        else:\n",
    "            # 링크 정보가 없는 경우 빈 문자열 할당\n",
    "            item['link'] = ''\n",
    "        return item\n",
    "\n",
    "class CleanTitlePipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        # \"관련 상품 추천\" 접미어 및 불필요한 개행문자 등 제거, None일 경우 빈 문자열 할당\n",
    "        if item['category'] is None:\n",
    "            item['category'] = ''\n",
    "        else:\n",
    "            item['category'] = item['category'].replace(\" 관련 상품 추천\", \"\").strip()\n",
    "        \n",
    "        # \"상품명: \" 접두어 및 불필요한 개행문자 등 제거, None일 경우 빈 문자열 할당\n",
    "        if item['title'] is None:\n",
    "            item['title'] = ''\n",
    "        else:\n",
    "            item['title'] = item['title'].replace(\"상품명: \", \"\").strip()\n",
    "        \n",
    "        # None일 경우 빈 문자열 할당\n",
    "        if item['name'] is None:\n",
    "            item['name'] = ''\n",
    "        else:\n",
    "            item['name'] = item['name'].strip()\n",
    "\n",
    "        if item['date'] is None:\n",
    "            item['date'] = ''\n",
    "        else:\n",
    "            item['date'] = item['date'].strip()\n",
    "\n",
    "        return item\n",
    "```\n",
    "\n",
    "- `settings.py`에서 `CleanTitlePipeline`을 활성화합니다.\n",
    "```python\n",
    "ITEM_PIPELINES = {\n",
    "    'daveleefun_project7.pipelines.CleanTitlePipeline': 300,\n",
    "    'daveleefun_project7.pipelines.LinkCompletionPipeline': 400\n",
    "}\n",
    "```\n",
    "\n",
    "#### 5. 크롤링 실행 및 결과 확인\n",
    "\n",
    "- 크롤링을 실행하고 결과를 확인합니다.\n",
    "```bash\n",
    "scrapy crawl multiple_webs -O multiple_webs_item.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cbf9df-4c59-4c71-84db-b44cd111fce3",
   "metadata": {},
   "source": [
    "### 참고: 주요 HTTP 응답 코드\n",
    "\n",
    "### 2xx (성공)\n",
    "\n",
    "- **200 OK**: 요청이 성공적으로 처리되었습니다. 가장 일반적인 성공 응답 코드입니다.\n",
    "- **201 Created**: 요청이 성공적으로 이루어져 새로운 리소스가 생성되었습니다. 예를 들어, POST 요청을 통해 새로운 엔트리가 데이터베이스에 추가되었을 때 사용됩니다.\n",
    "- **204 No Content**: 요청은 성공적이지만, 클라이언트에게 보내줄 콘텐츠는 없습니다. 예를 들어, DELETE 요청이 성공적으로 처리되었을 때 사용될 수 있습니다.\n",
    "\n",
    "### 3xx (리다이렉션)\n",
    "\n",
    "- **301 Moved Permanently**: 요청한 리소스가 영구적으로 새 위치로 이동했습니다. 이 코드는 리소스의 URL이 변경되었을 때 사용됩니다.\n",
    "- **302 Found**: 요청한 리소스가 일시적으로 다른 위치에 있음을 나타냅니다. 리다이렉션을 위해 자주 사용됩니다.\n",
    "\n",
    "### 4xx (클라이언트 오류)\n",
    "\n",
    "- **400 Bad Request**: 서버가 요청을 이해할 수 없음. 잘못된 요청 구조나 파라미터 때문에 발생할 수 있습니다.\n",
    "- **401 Unauthorized**: 요청이 인증을 필요로 함. 보통 로그인 하지 않은 사용자가 보호된 리소스에 접근하려 할 때 반환됩니다.\n",
    "- **403 Forbidden**: 서버가 요청을 이해했으나, 권한이 없어 요청을 거부합니다.\n",
    "- **404 Not Found**: 서버가 요청한 리소스를 찾을 수 없습니다. URL 오류나 존재하지 않는 페이지에 대한 요청에서 흔히 발생합니다.\n",
    "- **405 Method Not Allowed**: 요청된 리소스에서는 지원하지 않는 HTTP 메소드를 사용했습니다.\n",
    "- **422 Unprocessable Entity**: 요청은 이해했으나, 요청된 작업을 수행할 수 없음. 주로 요청 형식이 올바르지만, 요청 내용이 유효하지 않을 때 사용됩니다(예: 데이터 유효성 검증 실패).\n",
    "\n",
    "### 5xx (서버 오류)\n",
    "\n",
    "- **500 Internal Server Error**: 서버 내부 오류로 인해 요청을 처리할 수 없습니다. 가장 일반적인 서버 오류 응답입니다.\n",
    "- **503 Service Unavailable**: 서버가 일시적으로 요청을 처리할 수 없습니다. 일반적으로 서버 과부하나 유지 관리로 인해 발생합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01202ce5-e03e-4e03-b7d8-c72f7247aa09",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #455A64;background-color:#ECEFF1;\">\n",
    "본 자료 및 영상 컨텐츠는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 컨텐츠 및 컨텐츠 일부 문구등을 외부에 공개, 게시하는 것을 금지합니다. 특히 자료에 대해서는 저작권법을 엄격하게 적용하겠습니다.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
