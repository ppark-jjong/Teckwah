{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0572d5bd-8ee5-46ac-bb55-b1136497f727",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #455A64;background-color:#ECEFF1;\">\n",
    "본 자료 및 영상 컨텐츠는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 컨텐츠 및 컨텐츠 일부 문구등을 외부에 공개, 게시하는 것을 금지합니다. 특히 자료에 대해서는 저작권법을 엄격하게 적용하겠습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce353e8",
   "metadata": {},
   "source": [
    "## Scrapy Spider 코드 작성\n",
    "\n",
    "### 1. **Spider 파일 열기**\n",
    "   - `spiders` 디렉토리 내의 크롤러 파일(`daveleefun.py`)을 엽니다.\n",
    "\n",
    "### 2. **Spider 클래스 살펴보기**\n",
    "   - 크롤러 파일에는 `DaveleefunSpider` 클래스가 정의되어 있습니다.\n",
    "   - 이 클래스는 `scrapy.Spider`를 상속받아 만들어집니다.\n",
    "   - 클래스 내부에는 `name`, `allowed_domains`, `start_urls` 속성이 있습니다.\n",
    "     - `name`: 크롤러의 이름을 지정합니다.\n",
    "     - `allowed_domains`: 크롤링 허용 도메인 리스트를 지정합니다.\n",
    "     - `start_urls`: 크롤링을 시작할 URL 리스트를 지정합니다.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class DaveleefunSpider(scrapy.Spider):\n",
    "    name = \"daveleefun\"\n",
    "    allowed_domains = [\"davelee-fun.github.io\"]\n",
    "    start_urls = [\"https://davelee-fun.github.io\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a1593",
   "metadata": {},
   "source": [
    "### 3. **파싱 메서드 작성**\n",
    "\n",
    "- `parse` 메서드는 크롤링한 페이지의 응답을 처리하는 역할을 합니다.\n",
    "- 응답 객체(`response`)를 인자로 받아, 필요한 데이터를 추출하고 처리합니다.\n",
    "\n",
    "- **`response.css`와 `response.xpath` 사용법**\n",
    "  - `response.css('selector::text').get()`: CSS 선택자를 사용하여 텍스트를 추출합니다. `::text`는 선택한 요소의 텍스트만을 추출합니다.\n",
    "  - `response.xpath('xpath').get()`: XPath를 사용하여 요소를 선택하고 텍스트를 추출합니다. `text()` 함수를 사용하여 텍스트 내용을 가져올 수 있습니다.\n",
    "  - `.get()`: 선택한 요소의 첫 번째 결과를 반환합니다. 여러 요소가 선택되는 경우 첫 번째 요소만 반환하며, 결과가 없으면 `None`을 반환합니다.\n",
    "  - `.getall()`: 선택한 모든 요소의 리스트를 반환합니다. 여러 요소를 선택한 경우 사용하며, 결과가 없으면 빈 리스트를 반환합니다.\n",
    "\n",
    "- 예시:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class DaveleefunSpider(scrapy.Spider):\n",
    "    name = \"daveleefun\"\n",
    "    allowed_domains = [\"davelee-fun.github.io\"]\n",
    "    start_urls = [\"https://davelee-fun.github.io\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # CSS 셀렉터를 사용하여 제목 추출\n",
    "        title = response.css('h1.sitetitle::text').get()\n",
    "        \n",
    "        # XPath를 사용하여 설명 추출\n",
    "        description = response.xpath('//p[@class=\"lead\"]/text()').get()\n",
    "        \n",
    "        # 추출한 데이터를 딕셔너리 형태로 저장\n",
    "        yield {\n",
    "            'title': title,\n",
    "            'description': description\n",
    "        }\n",
    "```\n",
    "\n",
    "- 위 코드에서 `response.css('h1.sitetitle::text').get()`는 CSS 셀렉터를 사용하여 `<h1 class=\"sitetitle\">`에 해당하는 요소를 선택하고, `::text`를 사용하여 해당 요소의 텍스트 값을 추출합니다.\n",
    "- `response.xpath('//p[@class=\"lead\"]/text()').get()`는 XPath를 사용하여 `<p class=\"lead\">`에 해당하는 요소를 선택하고, `text()`를 사용하여 해당 요소의 텍스트 값을 추출합니다.\n",
    "- 이렇게 추출한 텍스트 값은 `title`과 `description` 변수에 할당되고, 딕셔너리 형태로 저장되어 크롤링 결과로 반환됩니다.\n",
    "\n",
    "> 위와 같이 코드 작성 후, 결과를 저장할 폴더에서 `scrapy crawl daveleefun -o results.json` 실행\n",
    "\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class DaveleefunSpider(scrapy.Spider):\n",
    "    name = \"daveleefun\"\n",
    "    allowed_domains = [\"davelee-fun.github.io\"]\n",
    "    start_urls = [\"https://davelee-fun.github.io\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # CSS 셀렉터를 사용하여 제목 추출\n",
    "        title = response.css('h1.sitetitle::text').get()\n",
    "\n",
    "        # XPath를 사용하여 설명 추출\n",
    "        description = response.xpath('//p[@class=\"lead\"]/text()').get()\n",
    "\n",
    "        # 추출한 데이터를 딕셔너리 형태로 저장\n",
    "        yield {\n",
    "        'title': title,\n",
    "        'description': description.strip() # 데이터 전처리\n",
    "        }\n",
    "```\n",
    "\n",
    "- 위와 같이 코드 작성 후, 결과를 저장할 폴더에서 `scrapy crawl daveleefun -o results.json` 실행하면, 기존 데이터에 신규 데이터가 추가됨 \n",
    "- 기존의 results.json 파일을 덮어쓰면서 새로운 데이터를 저장하려면 크롤링 실행 명령어에 -O 옵션을 사용하면 됨\n",
    "- `-O` 옵션은 `--overwrite-output` 의 약어로, 지정한 출력 파일이 이미 존재하는 경우 덮어쓰도록 명령\n",
    "- 예: `scrapy crawl daveleefun -O results.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a08a14",
   "metadata": {},
   "source": [
    "### 4. **아이템 정의 및 사용**\n",
    "\n",
    "> Scrapy 프레임워크 사용법에서는 items.py 에서 크롤링할 데이터 구조를 선언하고, 이에 맞추어 parse() 에서 크롤링하는 것을 추천함\n",
    "\n",
    "- `items.py` 파일은 크롤링한 데이터를 구조화하고 관리하기 위한 역할을 합니다.\n",
    "- Scrapy의 공식 문서에 따르면, 아이템은 크롤링된 데이터를 담는 컨테이너로 사용되며, 파이썬의 딕셔너리와 유사한 API를 제공합니다.\n",
    "- 아이템을 정의하면 코드의 가독성과 유지보수성이 향상되며, 데이터의 일관성을 유지할 수 있습니다.\n",
    "- `parse` 메서드에서 직접 딕셔너리를 사용하여 데이터를 반환하는 대신, 아이템 객체를 사용하여 데이터를 저장하고 반환할 수 있습니다.\n",
    "- 아이템을 사용하면 크롤링된 데이터에 대한 유효성 검사, 후처리, 저장 등의 작업을 별도로 수행할 수 있습니다.\n",
    "\n",
    "#### 4.1. **`Field`의 역할과 사용법**\n",
    "\n",
    "- `Field`는 Scrapy의 `Item` 클래스에서 각 필드를 정의하는 데 사용됩니다.\n",
    "- 예시:\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class DaveleefunItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    description = scrapy.Field()\n",
    "```\n",
    "- 위 예시에서 `title`과 `description` 필드는 `Field`를 사용하여 정의됩니다.\n",
    "\n",
    "#### 4.2. **`Item` 클래스의 사용법**\n",
    "\n",
    "- `Item` 클래스는 크롤링된 데이터를 저장하고 관리하는 데 사용됩니다.\n",
    "- `Item` 클래스를 상속받아 사용자 정의 아이템 클래스를 만들 수 있습니다.\n",
    "- 사용자 정의 아이템 클래스에서 필드를 정의할 때 `Field`를 사용합니다.\n",
    "- `Item` 객체를 생성하고 필드에 값을 할당하여 크롤링된 데이터를 저장합니다.\n",
    "- `Item` 객체는 딕셔너리와 유사한 방식으로 접근할 수 있습니다.\n",
    "- 예시:\n",
    "```python\n",
    "item = DaveleefunItem()\n",
    "item['title'] = response.css('h1.sitetitle::text').get()\n",
    "item['description'] = response.xpath('//p[@class=\"lead\"]/text()').get()\n",
    "yield item\n",
    "```\n",
    "- 위 예시에서 `DaveleefunItem` 객체를 생성하고, `title`과 `description` 필드에 크롤링된 데이터를 할당합니다.\n",
    "- `yield` 키워드를 사용하여 `Item` 객체를 반환합니다.\n",
    "\n",
    "#### 4.3. **프로젝트 작성 및 실행**\n",
    "\n",
    "아래는 위 코드를 테스트하기 위해 필요한 프로젝트와 spider를 생성하고 실행하는 명령어입니다. 크롤링할 사이트는 \"https://davelee-fun.github.io\"입니다.\n",
    "\n",
    "1. 프로젝트 생성\n",
    "```\n",
    "scrapy startproject daveleefun_project1\n",
    "```\n",
    "\n",
    "2. 프로젝트 디렉토리로 이동\n",
    "```\n",
    "cd daveleefun_project1\n",
    "```\n",
    "\n",
    "3. Spider 생성\n",
    "```\n",
    "scrapy genspider daveleefun davelee-fun.github.io\n",
    "```\n",
    "\n",
    "4. `items.py` 파일 수정\n",
    "```python\n",
    "# items.py\n",
    "import scrapy\n",
    "\n",
    "class DaveleefunItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    description = scrapy.Field()\n",
    "```\n",
    "\n",
    "5. `daveleefun.py` 파일 수정\n",
    "```python\n",
    "# daveleefun.py\n",
    "import scrapy\n",
    "from daveleefun_project1.items import DaveleefunItem\n",
    "\n",
    "class DaveleefunSpider(scrapy.Spider):\n",
    "    name = 'daveleefun'\n",
    "    allowed_domains = ['davelee-fun.github.io']\n",
    "    start_urls = ['https://davelee-fun.github.io/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        item = DaveleefunItem()\n",
    "        item['title'] = response.css('h1.sitetitle::text').get()\n",
    "        description = response.xpath('//p[@class=\"lead\"]/text()').get()\n",
    "        item['description'] = description.strip()\n",
    "        yield item\n",
    "```\n",
    "\n",
    "6. 크롤링 실행\n",
    "```\n",
    "scrapy crawl daveleefun -O results.json\n",
    "```\n",
    "\n",
    "7. 크롤링 결과 확인\n",
    "- `results.json` 파일을 열어 크롤링된 데이터 확인\n",
    "\n",
    "위 명령어를 순서대로 실행하면 프로젝트를 생성하고, Spider를 정의하며, `Item` 클래스를 사용하여 크롤링된 데이터를 저장하고 관리할 수 있습니다.\n",
    "\n",
    "### 5. **크롤링 실행 및 결과 확인**\n",
    "\n",
    "- 작성한 크롤러를 실행하고 결과를 확인합니다.\n",
    "- 터미널에서 `scrapy crawl daveleefun -O results.json` 명령어를 사용하여 크롤링을 실행하고 결과를 저장합니다.\n",
    "- `results.json` 파일을 열어 크롤링된 데이터를 확인합니다.\n",
    "\n",
    "위와 같이 `Field`를 사용하여 `Item` 클래스의 필드를 정의하고, `Item` 객체를 사용하여 크롤링된 데이터를 저장하고 관리할 수 있습니다. 또한 프로젝트 작성 및 실행 과정을 통해 실제로 크롤링을 수행하고 결과를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66a4fc",
   "metadata": {},
   "source": [
    "### 6. **다양한 포맷으로 크롤링 데이터 저장하기**\n",
    "\n",
    "- Scrapy는 크롤링된 데이터를 다양한 포맷으로 저장할 수 있는 기능을 제공합니다.\n",
    "  - 주로 사용되는 포맷은 CSV, XML, JSON 등이 있습니다.\n",
    "  - 이러한 포맷으로 데이터를 저장하면 다른 도구나 프로그램에서 쉽게 데이터를 읽어들이고 처리할 수 있습니다.\n",
    "\n",
    "- `daveleefun` 크롤러를 예시로 데이터 저장 방법을 살펴보겠습니다.\n",
    "  - JSON 포맷으로 저장하기:\n",
    "    ```\n",
    "    scrapy crawl daveleefun -o results.json\n",
    "    ```\n",
    "    - 위 명령어는 `daveleefun` 크롤러를 실행하고, 크롤링된 데이터를 `results.json` 파일로 저장합니다.\n",
    "    - `-o` 옵션은 출력 파일을 지정하는 옵션입니다.\n",
    "\n",
    "  - CSV 포맷으로 저장하기:\n",
    "    ```\n",
    "    scrapy crawl daveleefun -o results.csv\n",
    "    ```\n",
    "    - 위 명령어는 `daveleefun` 크롤러를 실행하고, 크롤링된 데이터를 `results.csv` 파일로 저장합니다.\n",
    "\n",
    "  - XML 포맷으로 저장하기:\n",
    "    ```\n",
    "    scrapy crawl daveleefun -o results.xml\n",
    "    ```\n",
    "    - 위 명령어는 `daveleefun` 크롤러를 실행하고, 크롤링된 데이터를 `results.xml` 파일로 저장합니다.\n",
    "\n",
    "- `-o` 옵션을 사용하여 저장할 파일명과 확장자를 지정할 수 있습니다.\n",
    "  - 파일 확장자에 따라 저장되는 포맷이 결정됩니다.\n",
    "\n",
    "- 저장된 데이터의 구조는 `Item` 클래스에 정의된 필드에 따라 결정됩니다.\n",
    "  - 예를 들어, `DaveleefunItem` 클래스에 `title`과 `description` 필드가 정의되어 있으므로, 저장된 데이터에는 각 아이템의 `title`과 `description` 값이 포함됩니다.\n",
    "\n",
    "- 데이터 저장 시 주의사항:\n",
    "  - 파일 확장자: 저장 포맷에 맞는 파일 확장자를 사용해야 합니다.\n",
    "    - 예를 들어, CSV 포맷은 `.csv`, XML 포맷은 `.xml`, JSON 포맷은 `.json` 확장자를 사용합니다.\n",
    "  - 데이터 인코딩: 데이터를 저장할 때 인코딩을 고려해야 합니다.\n",
    "    - 기본적으로 Scrapy는 UTF-8 인코딩을 사용하지만, 필요에 따라 다른 인코딩을 지정할 수 있습니다.\n",
    "  - 기존 파일 덮어쓰기:\n",
    "    - `-o` 옵션을 사용하면 기존 파일에 데이터가 추가되는 방식으로 저장됩니다.\n",
    "    - 매번 새로운 데이터로 파일을 덮어쓰려면 `-O` 옵션을 사용해야 합니다.\n",
    "\n",
    "> Scrapy의 데이터 저장 기능을 활용하면 크롤링된 데이터를 다양한 포맷으로 저장하고 관리할 수 있습니다. <br>\n",
    "> 저장된 데이터는 분석, 시각화, 머신러닝 등 다양한 용도로 활용될 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f47a7-b3d6-4891-84a6-b1f5a7e384a9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #455A64;background-color:#ECEFF1;\">\n",
    "본 자료 및 영상 컨텐츠는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 컨텐츠 및 컨텐츠 일부 문구등을 외부에 공개, 게시하는 것을 금지합니다. 특히 자료에 대해서는 저작권법을 엄격하게 적용하겠습니다.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
