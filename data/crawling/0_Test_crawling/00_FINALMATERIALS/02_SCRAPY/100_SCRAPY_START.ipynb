{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19477c22-a6e3-4514-9888-5c7397c4e3aa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #455A64;background-color:#ECEFF1;\">\n",
    "본 자료 및 영상 컨텐츠는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 컨텐츠 및 컨텐츠 일부 문구등을 외부에 공개, 게시하는 것을 금지합니다. 특히 자료에 대해서는 저작권법을 엄격하게 적용하겠습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e17de6",
   "metadata": {},
   "source": [
    "## 1. Scrapy 소개\n",
    "\n",
    "### Scrapy : 크롤링 프레임워크\n",
    "\n",
    "- 고급 기능을 제공하면서도 입문자도 쉽게 시작할 수 있음.\n",
    "\n",
    "### Scrapy 소개\n",
    "\n",
    "- Scrapy는 복잡해 보이지만, 기본을 익히고 나면 BeautifulSoup보다 사용하기 편리함.\n",
    "- 웹 크롤링에 필요한 다양한 기능을 포괄적으로 제공하는 대표적인 프레임워크.\n",
    "- 초보자도 필요한 기능을 빠르게 배우고 적용할 수 있도록 설계됨.\n",
    "\n",
    "### 프레임워크 특징\n",
    "\n",
    "- 이미 많은 기능이 구현되어 있어 사용자는 비즈니스 로직에 집중할 수 있음.\n",
    "- 크롤링 작업을 위한 표준화된 방법을 제공하여 개발자가 쉽게 크롤러를 구축할 수 있게 도움.\n",
    "- Python을 비롯한 다양한 프로그래밍 언어에 적용 가능한 개념.\n",
    "- 학습 곡선이 있으나, 한번 익히면 강력한 도구로 활용 가능.\n",
    "\n",
    "### Scrapy 장점\n",
    "\n",
    "- 안정적인 크롤링: 내부적으로 다양한 오류 처리와 안정성을 위한 기능을 포함.\n",
    "- 빠른 크롤링: 동시성을 지원하여 대량의 데이터도 신속하게 처리 가능.\n",
    "- 다양한 기능: 데이터를 여러 포맷으로 쉽게 저장하고, 크롤링 과정을 세밀하게 조정할 수 있는 기능 제공.\n",
    "\n",
    "### 사용 방법 요약\n",
    "\n",
    "1. Scrapy를 이용하여 크롤링할 대상의 스파이더를 생성.\n",
    "2. 대상 웹사이트와 추출할 데이터에 대한 선택자(selector)를 정의.\n",
    "3. 설정에 따라 크롤러를 실행하여 데이터 수집."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c2eec",
   "metadata": {},
   "source": [
    "## 2. 개발 환경 준비\n",
    "\n",
    "\n",
    "### 아나콘다 (Python) 설치하기\n",
    "- Scrapy는 Python 기반으로 동작하므로 Python이 설치되어 있어야 함.\n",
    "\n",
    "### Scrapy 설치하기\n",
    "\n",
    "- Python 설치 후, pip(파이썬 패키지 관리자)을 이용해 Scrapy 설치.\n",
    "- 터미널이나 커맨드 프롬프트에서 \"pip install scrapy\" 명령어 실행.\n",
    "  - 설치 후, 터미널을 오픈하여, scrapy 정상 동작 테스트 필요\n",
    "  - 정상 동작하지 않을 경우, `!conda install scrapy -y` 로 설치 추천 \n",
    "    - ! (느낌표) 는 주피터 노트북에서 터미널 명령 실행시 필요\n",
    "    - conda 명령은 아나콘다 폴더 내에 라이브러리를 설치하는 명령으로 아나콘다 폴더는 적어도 환경변수(PATH)에 설정되어 있을 것을 기대하기 때문임\n",
    "    \n",
    "> 윈도우에서는 터미널 오픈시 관리자 권한으로 오픈 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25e4ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a90f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install scrapy -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98f1c4",
   "metadata": {},
   "source": [
    "### 개발 환경 설정\n",
    "\n",
    "- 코드 편집기(예: Visual Studio Code) 설치 권장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c7b96",
   "metadata": {},
   "source": [
    "### 크롤링 프로젝트 생성\n",
    "\n",
    "1. VSCode 실행 및 터미널 오픈 (작업 폴더 선택)\n",
    "2. Scrapy 프로젝트 생성 명령어\n",
    "   - `scrapy startproject <프로젝트 이름>` 사용.\n",
    "   ```python\n",
    "   scrapy startproject helloscrapy\n",
    "   ```\n",
    "3. 생성된 프로젝트 확인\n",
    "   - 생성된 폴더(helloscrapy)로 이동\n",
    "\n",
    "### Scrapy로 작성된 프로젝트 구조\n",
    "\n",
    "- `scrapy.cfg` : 배포 설정 파일. (어떻게 서버를 띄울것인지)\n",
    "- `helloscrapy/` : 프로젝트의 Python 모듈, 여기서 코드를 import.\n",
    "    - `__init__.py` : 디렉토리를 Python 패키지로 만들어 주는 역할. 이 파일이 있어야 Python은 해당 디렉토리를 패키지로 인식하고,\n",
    "      -> 모듈을 가져올 수 있음.             scrapy상으로 python 파일들이 디렉토리 처럼 여겨 지므로 패키지로 인식해 실행될 수 있도록 도움을 줌 \n",
    "    - `items.py` : 프로젝트 아이템 정의 파일. 크롤링할 데이터의 구조(모델)를 정의.\n",
    "    - `pipelines.py` : 프로젝트 파이프라인 파일. 아이템을 처리하는 방법(예: 데이터 정제, 데이터베이스 저장) 정의.\n",
    "크롤링을 했을 때 크롤링 데이터를 처리하는 (추가, 삭제, 수정 등) 파일                     \n",
    "    - `settings.py` : 프로젝트 설정 파일. Scrapy 설정을 포함(예: 동시 요청 수, 미들웨어, 사용자 에이전트).\n",
    "    - `spiders/` : 크롤러(스파이더)를 저장할 디렉토리.\n",
    "        - `__init__.py` : 이 파일 또한 디렉토리를 Python 패키지로 만들어 주는 역할. spiders 디렉토리 안의 크롤러들을 모듈로서 사용 가능하게 함.\n",
    "    - 참고: `iddlewares.py` : 사용자가 정의한 미들웨어를 통해 Scrapy의 요청 및 응답 처리 과정에 개입할 수 있게 하는 역할을 함. 이를 통해 요청의 수정, 응답의 사전 처리, 에러 핸들링 등의 고급 기능을 구현할 수 있음 (일반적인 경우에는 사용할 일은 없음)\n",
    "\n",
    "\n",
    "### 크롤러(spider) 작성 방법\n",
    "\n",
    "- **터미널에서 크롤러 생성 명령어**\n",
    "   - 프로젝트 폴더 내에서 `scrapy genspider <크롤러 이름> <크롤링 페이지 주소>` 사용.\n",
    "   ```python\n",
    "   scrapy genspider daveleefun davelee-fun.github.io\n",
    "   ```\n",
    "   \n",
    "   - 실행 후, spiders 폴더에 가면, daveleefun.py 파일이 생성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc46dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 크롤링 실행 방법\n",
    "\n",
    "크롤러가 준비되면, 실제로 크롤링을 실행해 데이터를 수집하는 단계로 넘어갑니다.\n",
    "     <br/><br/>\n",
    "\n",
    "1. 크롤링 실행 명령어\n",
    "   - VSCode 터미널에서 크롤러를 실행하기 위한 명령어는 `scrapy crawl <크롤러 이름>`입니다.\n",
    "   ```python\n",
    "   scrapy crawl daveleefun\n",
    "   ```\n",
    "   - 이 명령어는 `daveleefun` 크롤러를 실행하며, 해당 크롤러가 지정한 웹페이지에서 데이터를 추출합니다.\n",
    "     <br/><br/>\n",
    "\n",
    "2. 크롤링 데이터 저장 (아직은 크롤링 코드를 추가하지 않았으므로, 수집한 데이터는 없음)\n",
    "   - 수집된 데이터를 파일로 저장하고 싶다면, 실행 명령어에 `-o` 옵션을 추가합니다.\n",
    "     <br/><br/>\n",
    "\n",
    "   ```python\n",
    "   scrapy crawl daveleefun -o results.json\n",
    "   ```\n",
    "     <br/>\n",
    "   - 위 명령어는 크롤링 결과를 JSON 형식의 `results.json` 파일로 저장합니다.\n",
    "   - 다른 포맷으로 저장하고 싶다면, `.csv`나 `.xml`과 같이 파일 확장자를 변경하여 사용할 수 있습니다.\n",
    "   - `-o` 옵션을 사용하면 크롤링 결과가 지정한 파일에 저장됩니다\n",
    "     <br/><br/>\n",
    "\n",
    "4. 크롤링 결과 확인\n",
    "   - 크롤링이 완료된 후, 지정한 파일(`results.json`)을 열어 크롤링된 데이터를 확인할 수 있습니다.\n",
    "   - 데이터가 정상적으로 수집되었는지, 형식에 맞게 저장되었는지 검토합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0221e",
   "metadata": {},
   "source": [
    "### Scrapy로 크롤링하기 위한 주요 순서와 명령 요약\n",
    "\n",
    "1. Scrapy 프로젝트 생성   \n",
    "   - 프로젝트 생성 명령: `scrapy startproject helloscrapy`\n",
    "     <br/><br/>\n",
    "2. 크롤러(spider) 생성\n",
    "   - 프로젝트 폴더 내 터미널에서 크롤러 생성 명령: `scrapy genspider daveleefun davelee-fun.github.io`\n",
    "   - 생성 확인: `spiders` 폴더 내 `daveleefun.py` 파일 확인   <br/><br/>\n",
    "\n",
    "3. 크롤링 실행\n",
    "   - 크롤링 할 파일을 코딩한 후,\n",
    "   - 터미널에서 크롤링 실행 명령: `scrapy crawl daveleefun`\n",
    "   - 데이터 저장 옵션: `-o` 옵션 사용 (예: `scrapy crawl daveleefun -o results.json`)\n",
    "   - 결과 확인: `results.json` 파일 열어보기   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8833f1-40a9-49eb-a8c5-4799135f0f51",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #455A64;background-color:#ECEFF1;\">\n",
    "본 자료 및 영상 컨텐츠는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 컨텐츠 및 컨텐츠 일부 문구등을 외부에 공개, 게시하는 것을 금지합니다. 특히 자료에 대해서는 저작권법을 엄격하게 적용하겠습니다.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
